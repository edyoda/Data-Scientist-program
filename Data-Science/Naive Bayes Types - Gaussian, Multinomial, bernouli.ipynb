{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "* Gaussian Naive Bayes\n",
    "* Multinomial Naive Bayes\n",
    "* Bernoulli's Naive Bayes\n",
    "* Introduction to Kaggle\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "* Default Naive Bayes is all about categorical data.\n",
    "* If your data in continues & normal distribution, Gaussian Naive Bayes have to be used.\n",
    "* Naive Bayes is all about classification\n",
    "* Logistic Regression starts not doing well when dimension of datset is very large.\n",
    "* Naive Bayes work's very well on high dimensional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8dd2423198>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(cancer.data[:,0]).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(cancer.data, cancer.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45,  6],\n",
       "       [ 1, 91]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred=y_pred, y_true=testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951048951048951"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awantik/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.972027972027972"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = pd.DataFrame(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323782</td>\n",
       "      <td>0.997855</td>\n",
       "      <td>0.987357</td>\n",
       "      <td>0.170581</td>\n",
       "      <td>0.506124</td>\n",
       "      <td>0.676764</td>\n",
       "      <td>0.822529</td>\n",
       "      <td>0.147741</td>\n",
       "      <td>-0.311631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969539</td>\n",
       "      <td>0.297008</td>\n",
       "      <td>0.965137</td>\n",
       "      <td>0.941082</td>\n",
       "      <td>0.119616</td>\n",
       "      <td>0.413463</td>\n",
       "      <td>0.526911</td>\n",
       "      <td>0.744214</td>\n",
       "      <td>0.163953</td>\n",
       "      <td>0.007066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.323782</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.329533</td>\n",
       "      <td>0.321086</td>\n",
       "      <td>-0.023389</td>\n",
       "      <td>0.236702</td>\n",
       "      <td>0.302418</td>\n",
       "      <td>0.293464</td>\n",
       "      <td>0.071401</td>\n",
       "      <td>-0.076437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352573</td>\n",
       "      <td>0.912045</td>\n",
       "      <td>0.358040</td>\n",
       "      <td>0.343546</td>\n",
       "      <td>0.077503</td>\n",
       "      <td>0.277830</td>\n",
       "      <td>0.301025</td>\n",
       "      <td>0.295316</td>\n",
       "      <td>0.105008</td>\n",
       "      <td>0.119205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.997855</td>\n",
       "      <td>0.329533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986507</td>\n",
       "      <td>0.207278</td>\n",
       "      <td>0.556936</td>\n",
       "      <td>0.716136</td>\n",
       "      <td>0.850977</td>\n",
       "      <td>0.183027</td>\n",
       "      <td>-0.261477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.969476</td>\n",
       "      <td>0.303038</td>\n",
       "      <td>0.970387</td>\n",
       "      <td>0.941550</td>\n",
       "      <td>0.150549</td>\n",
       "      <td>0.455774</td>\n",
       "      <td>0.563879</td>\n",
       "      <td>0.771241</td>\n",
       "      <td>0.189115</td>\n",
       "      <td>0.051019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.987357</td>\n",
       "      <td>0.321086</td>\n",
       "      <td>0.986507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>0.498502</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>0.823269</td>\n",
       "      <td>0.151293</td>\n",
       "      <td>-0.283110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.962746</td>\n",
       "      <td>0.287489</td>\n",
       "      <td>0.959120</td>\n",
       "      <td>0.959213</td>\n",
       "      <td>0.123523</td>\n",
       "      <td>0.390410</td>\n",
       "      <td>0.512606</td>\n",
       "      <td>0.722017</td>\n",
       "      <td>0.143570</td>\n",
       "      <td>0.003738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.170581</td>\n",
       "      <td>-0.023389</td>\n",
       "      <td>0.207278</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659123</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.553695</td>\n",
       "      <td>0.557775</td>\n",
       "      <td>0.584792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213120</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>0.238853</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0.805324</td>\n",
       "      <td>0.472468</td>\n",
       "      <td>0.434926</td>\n",
       "      <td>0.503053</td>\n",
       "      <td>0.394309</td>\n",
       "      <td>0.499316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.506124</td>\n",
       "      <td>0.236702</td>\n",
       "      <td>0.556936</td>\n",
       "      <td>0.498502</td>\n",
       "      <td>0.659123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883121</td>\n",
       "      <td>0.831135</td>\n",
       "      <td>0.602641</td>\n",
       "      <td>0.565369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535315</td>\n",
       "      <td>0.248133</td>\n",
       "      <td>0.590210</td>\n",
       "      <td>0.509604</td>\n",
       "      <td>0.565541</td>\n",
       "      <td>0.865809</td>\n",
       "      <td>0.816275</td>\n",
       "      <td>0.815573</td>\n",
       "      <td>0.510223</td>\n",
       "      <td>0.687382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.676764</td>\n",
       "      <td>0.302418</td>\n",
       "      <td>0.716136</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.883121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921391</td>\n",
       "      <td>0.500667</td>\n",
       "      <td>0.336783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688236</td>\n",
       "      <td>0.299879</td>\n",
       "      <td>0.729565</td>\n",
       "      <td>0.675987</td>\n",
       "      <td>0.448822</td>\n",
       "      <td>0.754968</td>\n",
       "      <td>0.884103</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.409464</td>\n",
       "      <td>0.514930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.822529</td>\n",
       "      <td>0.293464</td>\n",
       "      <td>0.850977</td>\n",
       "      <td>0.823269</td>\n",
       "      <td>0.553695</td>\n",
       "      <td>0.831135</td>\n",
       "      <td>0.921391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.462497</td>\n",
       "      <td>0.166917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830318</td>\n",
       "      <td>0.292752</td>\n",
       "      <td>0.855923</td>\n",
       "      <td>0.809630</td>\n",
       "      <td>0.452753</td>\n",
       "      <td>0.667454</td>\n",
       "      <td>0.752399</td>\n",
       "      <td>0.910155</td>\n",
       "      <td>0.375744</td>\n",
       "      <td>0.368661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.147741</td>\n",
       "      <td>0.071401</td>\n",
       "      <td>0.183027</td>\n",
       "      <td>0.151293</td>\n",
       "      <td>0.557775</td>\n",
       "      <td>0.602641</td>\n",
       "      <td>0.500667</td>\n",
       "      <td>0.462497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185728</td>\n",
       "      <td>0.090651</td>\n",
       "      <td>0.219169</td>\n",
       "      <td>0.177193</td>\n",
       "      <td>0.426675</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.433721</td>\n",
       "      <td>0.430297</td>\n",
       "      <td>0.699826</td>\n",
       "      <td>0.438413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.311631</td>\n",
       "      <td>-0.076437</td>\n",
       "      <td>-0.261477</td>\n",
       "      <td>-0.283110</td>\n",
       "      <td>0.584792</td>\n",
       "      <td>0.565369</td>\n",
       "      <td>0.336783</td>\n",
       "      <td>0.166917</td>\n",
       "      <td>0.479921</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253691</td>\n",
       "      <td>-0.051269</td>\n",
       "      <td>-0.205151</td>\n",
       "      <td>-0.231854</td>\n",
       "      <td>0.504942</td>\n",
       "      <td>0.458798</td>\n",
       "      <td>0.346234</td>\n",
       "      <td>0.175325</td>\n",
       "      <td>0.334019</td>\n",
       "      <td>0.767297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.679090</td>\n",
       "      <td>0.275869</td>\n",
       "      <td>0.691765</td>\n",
       "      <td>0.732562</td>\n",
       "      <td>0.301467</td>\n",
       "      <td>0.497473</td>\n",
       "      <td>0.631925</td>\n",
       "      <td>0.698050</td>\n",
       "      <td>0.303379</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.715065</td>\n",
       "      <td>0.194799</td>\n",
       "      <td>0.719684</td>\n",
       "      <td>0.751548</td>\n",
       "      <td>0.141919</td>\n",
       "      <td>0.287103</td>\n",
       "      <td>0.380585</td>\n",
       "      <td>0.531062</td>\n",
       "      <td>0.094543</td>\n",
       "      <td>0.049559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.097317</td>\n",
       "      <td>0.386358</td>\n",
       "      <td>-0.086761</td>\n",
       "      <td>-0.066280</td>\n",
       "      <td>0.068406</td>\n",
       "      <td>0.046205</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.021480</td>\n",
       "      <td>0.128053</td>\n",
       "      <td>0.164174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111690</td>\n",
       "      <td>0.409003</td>\n",
       "      <td>-0.102242</td>\n",
       "      <td>-0.083195</td>\n",
       "      <td>-0.073658</td>\n",
       "      <td>-0.092439</td>\n",
       "      <td>-0.068956</td>\n",
       "      <td>-0.119638</td>\n",
       "      <td>-0.128215</td>\n",
       "      <td>-0.045655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.674172</td>\n",
       "      <td>0.281673</td>\n",
       "      <td>0.693135</td>\n",
       "      <td>0.726628</td>\n",
       "      <td>0.296092</td>\n",
       "      <td>0.548905</td>\n",
       "      <td>0.660391</td>\n",
       "      <td>0.710650</td>\n",
       "      <td>0.313893</td>\n",
       "      <td>0.039830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.697201</td>\n",
       "      <td>0.200371</td>\n",
       "      <td>0.721031</td>\n",
       "      <td>0.730713</td>\n",
       "      <td>0.130054</td>\n",
       "      <td>0.341919</td>\n",
       "      <td>0.418899</td>\n",
       "      <td>0.554897</td>\n",
       "      <td>0.109930</td>\n",
       "      <td>0.085433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.735864</td>\n",
       "      <td>0.259845</td>\n",
       "      <td>0.744983</td>\n",
       "      <td>0.800086</td>\n",
       "      <td>0.246552</td>\n",
       "      <td>0.455653</td>\n",
       "      <td>0.617427</td>\n",
       "      <td>0.690299</td>\n",
       "      <td>0.223970</td>\n",
       "      <td>-0.090170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.757373</td>\n",
       "      <td>0.196497</td>\n",
       "      <td>0.761213</td>\n",
       "      <td>0.811408</td>\n",
       "      <td>0.125389</td>\n",
       "      <td>0.283257</td>\n",
       "      <td>0.385100</td>\n",
       "      <td>0.538166</td>\n",
       "      <td>0.074126</td>\n",
       "      <td>0.017539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.222600</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>-0.202694</td>\n",
       "      <td>-0.166777</td>\n",
       "      <td>0.332375</td>\n",
       "      <td>0.135299</td>\n",
       "      <td>0.098564</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.187321</td>\n",
       "      <td>0.401964</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230691</td>\n",
       "      <td>-0.074743</td>\n",
       "      <td>-0.217304</td>\n",
       "      <td>-0.182195</td>\n",
       "      <td>0.314457</td>\n",
       "      <td>-0.055558</td>\n",
       "      <td>-0.058298</td>\n",
       "      <td>-0.102007</td>\n",
       "      <td>-0.107342</td>\n",
       "      <td>0.101480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.191975</td>\n",
       "      <td>0.250744</td>\n",
       "      <td>0.212583</td>\n",
       "      <td>0.318943</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.670279</td>\n",
       "      <td>0.490424</td>\n",
       "      <td>0.421659</td>\n",
       "      <td>0.559837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204607</td>\n",
       "      <td>0.143003</td>\n",
       "      <td>0.260516</td>\n",
       "      <td>0.199371</td>\n",
       "      <td>0.227394</td>\n",
       "      <td>0.678780</td>\n",
       "      <td>0.639147</td>\n",
       "      <td>0.483208</td>\n",
       "      <td>0.277878</td>\n",
       "      <td>0.590973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.194204</td>\n",
       "      <td>0.143293</td>\n",
       "      <td>0.228082</td>\n",
       "      <td>0.207660</td>\n",
       "      <td>0.248396</td>\n",
       "      <td>0.570517</td>\n",
       "      <td>0.691270</td>\n",
       "      <td>0.439167</td>\n",
       "      <td>0.342627</td>\n",
       "      <td>0.446630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186904</td>\n",
       "      <td>0.100241</td>\n",
       "      <td>0.226680</td>\n",
       "      <td>0.188353</td>\n",
       "      <td>0.168481</td>\n",
       "      <td>0.484858</td>\n",
       "      <td>0.662564</td>\n",
       "      <td>0.440472</td>\n",
       "      <td>0.197788</td>\n",
       "      <td>0.439329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.376169</td>\n",
       "      <td>0.163851</td>\n",
       "      <td>0.407217</td>\n",
       "      <td>0.372320</td>\n",
       "      <td>0.380676</td>\n",
       "      <td>0.642262</td>\n",
       "      <td>0.683260</td>\n",
       "      <td>0.615634</td>\n",
       "      <td>0.393298</td>\n",
       "      <td>0.341198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358127</td>\n",
       "      <td>0.086741</td>\n",
       "      <td>0.394999</td>\n",
       "      <td>0.342271</td>\n",
       "      <td>0.215351</td>\n",
       "      <td>0.452888</td>\n",
       "      <td>0.549592</td>\n",
       "      <td>0.602450</td>\n",
       "      <td>0.143116</td>\n",
       "      <td>0.310655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.104321</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>-0.081629</td>\n",
       "      <td>-0.072497</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>0.229977</td>\n",
       "      <td>0.178009</td>\n",
       "      <td>0.095351</td>\n",
       "      <td>0.449137</td>\n",
       "      <td>0.345007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128121</td>\n",
       "      <td>-0.077473</td>\n",
       "      <td>-0.103753</td>\n",
       "      <td>-0.110343</td>\n",
       "      <td>-0.012662</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>0.037119</td>\n",
       "      <td>-0.030413</td>\n",
       "      <td>0.389402</td>\n",
       "      <td>0.078079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.042641</td>\n",
       "      <td>0.054458</td>\n",
       "      <td>-0.005523</td>\n",
       "      <td>-0.019887</td>\n",
       "      <td>0.283607</td>\n",
       "      <td>0.507318</td>\n",
       "      <td>0.449301</td>\n",
       "      <td>0.257584</td>\n",
       "      <td>0.331786</td>\n",
       "      <td>0.688132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037488</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.022736</td>\n",
       "      <td>0.170568</td>\n",
       "      <td>0.390159</td>\n",
       "      <td>0.379975</td>\n",
       "      <td>0.215204</td>\n",
       "      <td>0.111094</td>\n",
       "      <td>0.591328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.969539</td>\n",
       "      <td>0.352573</td>\n",
       "      <td>0.969476</td>\n",
       "      <td>0.962746</td>\n",
       "      <td>0.213120</td>\n",
       "      <td>0.535315</td>\n",
       "      <td>0.688236</td>\n",
       "      <td>0.830318</td>\n",
       "      <td>0.185728</td>\n",
       "      <td>-0.253691</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.359921</td>\n",
       "      <td>0.993708</td>\n",
       "      <td>0.984015</td>\n",
       "      <td>0.216574</td>\n",
       "      <td>0.475820</td>\n",
       "      <td>0.573975</td>\n",
       "      <td>0.787424</td>\n",
       "      <td>0.243529</td>\n",
       "      <td>0.093492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.297008</td>\n",
       "      <td>0.912045</td>\n",
       "      <td>0.303038</td>\n",
       "      <td>0.287489</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>0.248133</td>\n",
       "      <td>0.299879</td>\n",
       "      <td>0.292752</td>\n",
       "      <td>0.090651</td>\n",
       "      <td>-0.051269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359921</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.365098</td>\n",
       "      <td>0.345842</td>\n",
       "      <td>0.225429</td>\n",
       "      <td>0.360832</td>\n",
       "      <td>0.368366</td>\n",
       "      <td>0.359755</td>\n",
       "      <td>0.233027</td>\n",
       "      <td>0.219122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.965137</td>\n",
       "      <td>0.358040</td>\n",
       "      <td>0.970387</td>\n",
       "      <td>0.959120</td>\n",
       "      <td>0.238853</td>\n",
       "      <td>0.590210</td>\n",
       "      <td>0.729565</td>\n",
       "      <td>0.855923</td>\n",
       "      <td>0.219169</td>\n",
       "      <td>-0.205151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.993708</td>\n",
       "      <td>0.365098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.236775</td>\n",
       "      <td>0.529408</td>\n",
       "      <td>0.618344</td>\n",
       "      <td>0.816322</td>\n",
       "      <td>0.269493</td>\n",
       "      <td>0.138957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.941082</td>\n",
       "      <td>0.343546</td>\n",
       "      <td>0.941550</td>\n",
       "      <td>0.959213</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0.509604</td>\n",
       "      <td>0.675987</td>\n",
       "      <td>0.809630</td>\n",
       "      <td>0.177193</td>\n",
       "      <td>-0.231854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.984015</td>\n",
       "      <td>0.345842</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209145</td>\n",
       "      <td>0.438296</td>\n",
       "      <td>0.543331</td>\n",
       "      <td>0.747419</td>\n",
       "      <td>0.209146</td>\n",
       "      <td>0.079647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.119616</td>\n",
       "      <td>0.077503</td>\n",
       "      <td>0.150549</td>\n",
       "      <td>0.123523</td>\n",
       "      <td>0.805324</td>\n",
       "      <td>0.565541</td>\n",
       "      <td>0.448822</td>\n",
       "      <td>0.452753</td>\n",
       "      <td>0.426675</td>\n",
       "      <td>0.504942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216574</td>\n",
       "      <td>0.225429</td>\n",
       "      <td>0.236775</td>\n",
       "      <td>0.209145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.568187</td>\n",
       "      <td>0.518523</td>\n",
       "      <td>0.547691</td>\n",
       "      <td>0.493838</td>\n",
       "      <td>0.617624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.413463</td>\n",
       "      <td>0.277830</td>\n",
       "      <td>0.455774</td>\n",
       "      <td>0.390410</td>\n",
       "      <td>0.472468</td>\n",
       "      <td>0.865809</td>\n",
       "      <td>0.754968</td>\n",
       "      <td>0.667454</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.458798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475820</td>\n",
       "      <td>0.360832</td>\n",
       "      <td>0.529408</td>\n",
       "      <td>0.438296</td>\n",
       "      <td>0.568187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892261</td>\n",
       "      <td>0.801080</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>0.810455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.526911</td>\n",
       "      <td>0.301025</td>\n",
       "      <td>0.563879</td>\n",
       "      <td>0.512606</td>\n",
       "      <td>0.434926</td>\n",
       "      <td>0.816275</td>\n",
       "      <td>0.884103</td>\n",
       "      <td>0.752399</td>\n",
       "      <td>0.433721</td>\n",
       "      <td>0.346234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573975</td>\n",
       "      <td>0.368366</td>\n",
       "      <td>0.618344</td>\n",
       "      <td>0.543331</td>\n",
       "      <td>0.518523</td>\n",
       "      <td>0.892261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.855434</td>\n",
       "      <td>0.532520</td>\n",
       "      <td>0.686511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.744214</td>\n",
       "      <td>0.295316</td>\n",
       "      <td>0.771241</td>\n",
       "      <td>0.722017</td>\n",
       "      <td>0.503053</td>\n",
       "      <td>0.815573</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.910155</td>\n",
       "      <td>0.430297</td>\n",
       "      <td>0.175325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787424</td>\n",
       "      <td>0.359755</td>\n",
       "      <td>0.816322</td>\n",
       "      <td>0.747419</td>\n",
       "      <td>0.547691</td>\n",
       "      <td>0.801080</td>\n",
       "      <td>0.855434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.502528</td>\n",
       "      <td>0.511114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.163953</td>\n",
       "      <td>0.105008</td>\n",
       "      <td>0.189115</td>\n",
       "      <td>0.143570</td>\n",
       "      <td>0.394309</td>\n",
       "      <td>0.510223</td>\n",
       "      <td>0.409464</td>\n",
       "      <td>0.375744</td>\n",
       "      <td>0.699826</td>\n",
       "      <td>0.334019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243529</td>\n",
       "      <td>0.233027</td>\n",
       "      <td>0.269493</td>\n",
       "      <td>0.209146</td>\n",
       "      <td>0.493838</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>0.532520</td>\n",
       "      <td>0.502528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.537848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.119205</td>\n",
       "      <td>0.051019</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.499316</td>\n",
       "      <td>0.687382</td>\n",
       "      <td>0.514930</td>\n",
       "      <td>0.368661</td>\n",
       "      <td>0.438413</td>\n",
       "      <td>0.767297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093492</td>\n",
       "      <td>0.219122</td>\n",
       "      <td>0.138957</td>\n",
       "      <td>0.079647</td>\n",
       "      <td>0.617624</td>\n",
       "      <td>0.810455</td>\n",
       "      <td>0.686511</td>\n",
       "      <td>0.511114</td>\n",
       "      <td>0.537848</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000  0.323782  0.997855  0.987357  0.170581  0.506124  0.676764   \n",
       "1   0.323782  1.000000  0.329533  0.321086 -0.023389  0.236702  0.302418   \n",
       "2   0.997855  0.329533  1.000000  0.986507  0.207278  0.556936  0.716136   \n",
       "3   0.987357  0.321086  0.986507  1.000000  0.177028  0.498502  0.685983   \n",
       "4   0.170581 -0.023389  0.207278  0.177028  1.000000  0.659123  0.521984   \n",
       "5   0.506124  0.236702  0.556936  0.498502  0.659123  1.000000  0.883121   \n",
       "6   0.676764  0.302418  0.716136  0.685983  0.521984  0.883121  1.000000   \n",
       "7   0.822529  0.293464  0.850977  0.823269  0.553695  0.831135  0.921391   \n",
       "8   0.147741  0.071401  0.183027  0.151293  0.557775  0.602641  0.500667   \n",
       "9  -0.311631 -0.076437 -0.261477 -0.283110  0.584792  0.565369  0.336783   \n",
       "10  0.679090  0.275869  0.691765  0.732562  0.301467  0.497473  0.631925   \n",
       "11 -0.097317  0.386358 -0.086761 -0.066280  0.068406  0.046205  0.076218   \n",
       "12  0.674172  0.281673  0.693135  0.726628  0.296092  0.548905  0.660391   \n",
       "13  0.735864  0.259845  0.744983  0.800086  0.246552  0.455653  0.617427   \n",
       "14 -0.222600  0.006614 -0.202694 -0.166777  0.332375  0.135299  0.098564   \n",
       "15  0.206000  0.191975  0.250744  0.212583  0.318943  0.738722  0.670279   \n",
       "16  0.194204  0.143293  0.228082  0.207660  0.248396  0.570517  0.691270   \n",
       "17  0.376169  0.163851  0.407217  0.372320  0.380676  0.642262  0.683260   \n",
       "18 -0.104321  0.009127 -0.081629 -0.072497  0.200774  0.229977  0.178009   \n",
       "19 -0.042641  0.054458 -0.005523 -0.019887  0.283607  0.507318  0.449301   \n",
       "20  0.969539  0.352573  0.969476  0.962746  0.213120  0.535315  0.688236   \n",
       "21  0.297008  0.912045  0.303038  0.287489  0.036072  0.248133  0.299879   \n",
       "22  0.965137  0.358040  0.970387  0.959120  0.238853  0.590210  0.729565   \n",
       "23  0.941082  0.343546  0.941550  0.959213  0.206718  0.509604  0.675987   \n",
       "24  0.119616  0.077503  0.150549  0.123523  0.805324  0.565541  0.448822   \n",
       "25  0.413463  0.277830  0.455774  0.390410  0.472468  0.865809  0.754968   \n",
       "26  0.526911  0.301025  0.563879  0.512606  0.434926  0.816275  0.884103   \n",
       "27  0.744214  0.295316  0.771241  0.722017  0.503053  0.815573  0.861323   \n",
       "28  0.163953  0.105008  0.189115  0.143570  0.394309  0.510223  0.409464   \n",
       "29  0.007066  0.119205  0.051019  0.003738  0.499316  0.687382  0.514930   \n",
       "\n",
       "          7         8         9   ...        20        21        22        23  \\\n",
       "0   0.822529  0.147741 -0.311631  ...  0.969539  0.297008  0.965137  0.941082   \n",
       "1   0.293464  0.071401 -0.076437  ...  0.352573  0.912045  0.358040  0.343546   \n",
       "2   0.850977  0.183027 -0.261477  ...  0.969476  0.303038  0.970387  0.941550   \n",
       "3   0.823269  0.151293 -0.283110  ...  0.962746  0.287489  0.959120  0.959213   \n",
       "4   0.553695  0.557775  0.584792  ...  0.213120  0.036072  0.238853  0.206718   \n",
       "5   0.831135  0.602641  0.565369  ...  0.535315  0.248133  0.590210  0.509604   \n",
       "6   0.921391  0.500667  0.336783  ...  0.688236  0.299879  0.729565  0.675987   \n",
       "7   1.000000  0.462497  0.166917  ...  0.830318  0.292752  0.855923  0.809630   \n",
       "8   0.462497  1.000000  0.479921  ...  0.185728  0.090651  0.219169  0.177193   \n",
       "9   0.166917  0.479921  1.000000  ... -0.253691 -0.051269 -0.205151 -0.231854   \n",
       "10  0.698050  0.303379  0.000111  ...  0.715065  0.194799  0.719684  0.751548   \n",
       "11  0.021480  0.128053  0.164174  ... -0.111690  0.409003 -0.102242 -0.083195   \n",
       "12  0.710650  0.313893  0.039830  ...  0.697201  0.200371  0.721031  0.730713   \n",
       "13  0.690299  0.223970 -0.090170  ...  0.757373  0.196497  0.761213  0.811408   \n",
       "14  0.027653  0.187321  0.401964  ... -0.230691 -0.074743 -0.217304 -0.182195   \n",
       "15  0.490424  0.421659  0.559837  ...  0.204607  0.143003  0.260516  0.199371   \n",
       "16  0.439167  0.342627  0.446630  ...  0.186904  0.100241  0.226680  0.188353   \n",
       "17  0.615634  0.393298  0.341198  ...  0.358127  0.086741  0.394999  0.342271   \n",
       "18  0.095351  0.449137  0.345007  ... -0.128121 -0.077473 -0.103753 -0.110343   \n",
       "19  0.257584  0.331786  0.688132  ... -0.037488 -0.003195 -0.001000 -0.022736   \n",
       "20  0.830318  0.185728 -0.253691  ...  1.000000  0.359921  0.993708  0.984015   \n",
       "21  0.292752  0.090651 -0.051269  ...  0.359921  1.000000  0.365098  0.345842   \n",
       "22  0.855923  0.219169 -0.205151  ...  0.993708  0.365098  1.000000  0.977578   \n",
       "23  0.809630  0.177193 -0.231854  ...  0.984015  0.345842  0.977578  1.000000   \n",
       "24  0.452753  0.426675  0.504942  ...  0.216574  0.225429  0.236775  0.209145   \n",
       "25  0.667454  0.473200  0.458798  ...  0.475820  0.360832  0.529408  0.438296   \n",
       "26  0.752399  0.433721  0.346234  ...  0.573975  0.368366  0.618344  0.543331   \n",
       "27  0.910155  0.430297  0.175325  ...  0.787424  0.359755  0.816322  0.747419   \n",
       "28  0.375744  0.699826  0.334019  ...  0.243529  0.233027  0.269493  0.209146   \n",
       "29  0.368661  0.438413  0.767297  ...  0.093492  0.219122  0.138957  0.079647   \n",
       "\n",
       "          24        25        26        27        28        29  \n",
       "0   0.119616  0.413463  0.526911  0.744214  0.163953  0.007066  \n",
       "1   0.077503  0.277830  0.301025  0.295316  0.105008  0.119205  \n",
       "2   0.150549  0.455774  0.563879  0.771241  0.189115  0.051019  \n",
       "3   0.123523  0.390410  0.512606  0.722017  0.143570  0.003738  \n",
       "4   0.805324  0.472468  0.434926  0.503053  0.394309  0.499316  \n",
       "5   0.565541  0.865809  0.816275  0.815573  0.510223  0.687382  \n",
       "6   0.448822  0.754968  0.884103  0.861323  0.409464  0.514930  \n",
       "7   0.452753  0.667454  0.752399  0.910155  0.375744  0.368661  \n",
       "8   0.426675  0.473200  0.433721  0.430297  0.699826  0.438413  \n",
       "9   0.504942  0.458798  0.346234  0.175325  0.334019  0.767297  \n",
       "10  0.141919  0.287103  0.380585  0.531062  0.094543  0.049559  \n",
       "11 -0.073658 -0.092439 -0.068956 -0.119638 -0.128215 -0.045655  \n",
       "12  0.130054  0.341919  0.418899  0.554897  0.109930  0.085433  \n",
       "13  0.125389  0.283257  0.385100  0.538166  0.074126  0.017539  \n",
       "14  0.314457 -0.055558 -0.058298 -0.102007 -0.107342  0.101480  \n",
       "15  0.227394  0.678780  0.639147  0.483208  0.277878  0.590973  \n",
       "16  0.168481  0.484858  0.662564  0.440472  0.197788  0.439329  \n",
       "17  0.215351  0.452888  0.549592  0.602450  0.143116  0.310655  \n",
       "18 -0.012662  0.060255  0.037119 -0.030413  0.389402  0.078079  \n",
       "19  0.170568  0.390159  0.379975  0.215204  0.111094  0.591328  \n",
       "20  0.216574  0.475820  0.573975  0.787424  0.243529  0.093492  \n",
       "21  0.225429  0.360832  0.368366  0.359755  0.233027  0.219122  \n",
       "22  0.236775  0.529408  0.618344  0.816322  0.269493  0.138957  \n",
       "23  0.209145  0.438296  0.543331  0.747419  0.209146  0.079647  \n",
       "24  1.000000  0.568187  0.518523  0.547691  0.493838  0.617624  \n",
       "25  0.568187  1.000000  0.892261  0.801080  0.614441  0.810455  \n",
       "26  0.518523  0.892261  1.000000  0.855434  0.532520  0.686511  \n",
       "27  0.547691  0.801080  0.855434  1.000000  0.502528  0.511114  \n",
       "28  0.493838  0.614441  0.532520  0.502528  1.000000  0.537848  \n",
       "29  0.617624  0.810455  0.686511  0.511114  0.537848  1.000000  \n",
       "\n",
       "[30 rows x 30 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cancer.data[:,3:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.hstack([cancer.data[:,:1], features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(features, cancer.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9300699300699301"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.score(testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes\n",
    "* Feature are counters or discrete in nature\n",
    "* Works very well with Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_data = pd.read_csv('https://raw.githubusercontent.com/edyoda/data-science-complete-tutorial/master/Data/horror-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19579 entries, 0 to 19578\n",
      "Data columns (total 3 columns):\n",
      "id        19579 non-null object\n",
      "text      19579 non-null object\n",
      "author    19579 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 459.0+ KB\n"
     ]
    }
   ],
   "source": [
    "horror_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8831</th>\n",
       "      <td>id26541</td>\n",
       "      <td>Yes, he had followed me in my travels; he had ...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2951</th>\n",
       "      <td>id09538</td>\n",
       "      <td>The same name the same contour of person the s...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15289</th>\n",
       "      <td>id27162</td>\n",
       "      <td>Polluted by crimes and torn by the bitterest r...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18659</th>\n",
       "      <td>id12507</td>\n",
       "      <td>Let them raise a mound above my lifeless body,...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18769</th>\n",
       "      <td>id12513</td>\n",
       "      <td>I trod heaven in my thoughts, now exulting in ...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author\n",
       "8831   id26541  Yes, he had followed me in my travels; he had ...    MWS\n",
       "2951   id09538  The same name the same contour of person the s...    EAP\n",
       "15289  id27162  Polluted by crimes and torn by the bitterest r...    MWS\n",
       "18659  id12507  Let them raise a mound above my lifeless body,...    MWS\n",
       "18769  id12513  I trod heaven in my thoughts, now exulting in ...    MWS"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horror_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EAP    7900\n",
       "MWS    6044\n",
       "HPL    5635\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horror_data.author.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to do some text preprocessing here \n",
    "* First step tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'process',\n",
       " 'however',\n",
       " 'afforded',\n",
       " 'me',\n",
       " 'no',\n",
       " 'means',\n",
       " 'of',\n",
       " 'ascertaining',\n",
       " 'the',\n",
       " 'dimensions',\n",
       " 'of',\n",
       " 'my',\n",
       " 'dungeon',\n",
       " 'as',\n",
       " 'I',\n",
       " 'might',\n",
       " 'make',\n",
       " 'its',\n",
       " 'circuit',\n",
       " 'and',\n",
       " 'return',\n",
       " 'to',\n",
       " 'the',\n",
       " 'point',\n",
       " 'whence',\n",
       " 'I',\n",
       " 'set',\n",
       " 'out',\n",
       " 'without',\n",
       " 'being',\n",
       " 'aware',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'so',\n",
       " 'perfectly',\n",
       " 'uniform',\n",
       " 'seemed',\n",
       " 'the',\n",
       " 'wall']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(horror_data.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_data['text_tokenized'] = horror_data.text.map(lambda t: tokenizer.tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14779</th>\n",
       "      <td>id11608</td>\n",
       "      <td>The vegetables in the gardens, the milk and ch...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[The, vegetables, in, the, gardens, the, milk,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12158</th>\n",
       "      <td>id26933</td>\n",
       "      <td>To all appearance it had been temporarily and ...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[To, all, appearance, it, had, been, temporari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6445</th>\n",
       "      <td>id06287</td>\n",
       "      <td>Perdita and her child were to remain at Kishan.</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[Perdita, and, her, child, were, to, remain, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3927</th>\n",
       "      <td>id03986</td>\n",
       "      <td>\"Yet I cannot ask you to renounce your country...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[Yet, I, cannot, ask, you, to, renounce, your,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6729</th>\n",
       "      <td>id16608</td>\n",
       "      <td>He half felt that he was followed a psychologi...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[He, half, felt, that, he, was, followed, a, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author  \\\n",
       "14779  id11608  The vegetables in the gardens, the milk and ch...    MWS   \n",
       "12158  id26933  To all appearance it had been temporarily and ...    EAP   \n",
       "6445   id06287    Perdita and her child were to remain at Kishan.    MWS   \n",
       "3927   id03986  \"Yet I cannot ask you to renounce your country...    MWS   \n",
       "6729   id16608  He half felt that he was followed a psychologi...    HPL   \n",
       "\n",
       "                                          text_tokenized  \n",
       "14779  [The, vegetables, in, the, gardens, the, milk,...  \n",
       "12158  [To, all, appearance, it, had, been, temporari...  \n",
       "6445   [Perdita, and, her, child, were, to, remain, a...  \n",
       "3927   [Yet, I, cannot, ask, you, to, renounce, your,...  \n",
       "6729   [He, half, felt, that, he, was, followed, a, p...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horror_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stemming for finding root words, this helps in reducing the dimension of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_data['text_stemmed'] = horror_data['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>id09912</td>\n",
       "      <td>Gilman mechanically attended classes that morn...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[Gilman, mechanically, attended, classes, that...</td>\n",
       "      <td>[gilman, mechan, attend, class, that, morn, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>id09566</td>\n",
       "      <td>By the utter simplicity, by the nakedness of h...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[By, the, utter, simplicity, by, the, nakednes...</td>\n",
       "      <td>[by, the, utter, simplic, by, the, naked, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19184</th>\n",
       "      <td>id23908</td>\n",
       "      <td>This time the creaking went along the hall and...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[This, time, the, creaking, went, along, the, ...</td>\n",
       "      <td>[this, time, the, creak, went, along, the, hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>id14569</td>\n",
       "      <td>All this I can still remember, though I no lon...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[All, this, I, can, still, remember, though, I...</td>\n",
       "      <td>[all, this, i, can, still, rememb, though, i, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19350</th>\n",
       "      <td>id26013</td>\n",
       "      <td>It was said that the swart men who dwelt in Th...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[It, was, said, that, the, swart, men, who, dw...</td>\n",
       "      <td>[it, was, said, that, the, swart, men, who, dw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author  \\\n",
       "585    id09912  Gilman mechanically attended classes that morn...    HPL   \n",
       "6171   id09566  By the utter simplicity, by the nakedness of h...    EAP   \n",
       "19184  id23908  This time the creaking went along the hall and...    HPL   \n",
       "2908   id14569  All this I can still remember, though I no lon...    HPL   \n",
       "19350  id26013  It was said that the swart men who dwelt in Th...    HPL   \n",
       "\n",
       "                                          text_tokenized  \\\n",
       "585    [Gilman, mechanically, attended, classes, that...   \n",
       "6171   [By, the, utter, simplicity, by, the, nakednes...   \n",
       "19184  [This, time, the, creaking, went, along, the, ...   \n",
       "2908   [All, this, I, can, still, remember, though, I...   \n",
       "19350  [It, was, said, that, the, swart, men, who, dw...   \n",
       "\n",
       "                                            text_stemmed  \n",
       "585    [gilman, mechan, attend, class, that, morn, bu...  \n",
       "6171   [by, the, utter, simplic, by, the, naked, of, ...  \n",
       "19184  [this, time, the, creak, went, along, the, hal...  \n",
       "2908   [all, this, i, can, still, rememb, though, i, ...  \n",
       "19350  [it, was, said, that, the, swart, men, who, dw...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horror_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, stemmed words need to be converted into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror_data['text_sent'] = horror_data['text_stemmed'].map(lambda l: ' '.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10610</th>\n",
       "      <td>id21810</td>\n",
       "      <td>She, however, shunned society, and, attaching ...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[She, however, shunned, society, and, attachin...</td>\n",
       "      <td>[she, howev, shun, societi, and, attach, herse...</td>\n",
       "      <td>she howev shun societi and attach herself to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11553</th>\n",
       "      <td>id20986</td>\n",
       "      <td>\"Suppose you detail,\" said I, \"the particulars...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>[Suppose, you, detail, said, I, the, particula...</td>\n",
       "      <td>[suppos, you, detail, said, i, the, particular...</td>\n",
       "      <td>suppos you detail said i the particular of you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6666</th>\n",
       "      <td>id19002</td>\n",
       "      <td>Lay down your arms, fellow men brethren Pardon...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[Lay, down, your, arms, fellow, men, brethren,...</td>\n",
       "      <td>[lay, down, your, arm, fellow, men, brethren, ...</td>\n",
       "      <td>lay down your arm fellow men brethren pardon s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9120</th>\n",
       "      <td>id26386</td>\n",
       "      <td>An advanced guard gave information of our appr...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>[An, advanced, guard, gave, information, of, o...</td>\n",
       "      <td>[an, advanc, guard, gave, inform, of, our, app...</td>\n",
       "      <td>an advanc guard gave inform of our approach an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>id10417</td>\n",
       "      <td>A paralysis of fear stifled all attempts to cr...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>[A, paralysis, of, fear, stifled, all, attempt...</td>\n",
       "      <td>[a, paralysi, of, fear, stifl, all, attempt, t...</td>\n",
       "      <td>a paralysi of fear stifl all attempt to cri out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text author  \\\n",
       "10610  id21810  She, however, shunned society, and, attaching ...    EAP   \n",
       "11553  id20986  \"Suppose you detail,\" said I, \"the particulars...    EAP   \n",
       "6666   id19002  Lay down your arms, fellow men brethren Pardon...    MWS   \n",
       "9120   id26386  An advanced guard gave information of our appr...    MWS   \n",
       "598    id10417  A paralysis of fear stifled all attempts to cr...    HPL   \n",
       "\n",
       "                                          text_tokenized  \\\n",
       "10610  [She, however, shunned, society, and, attachin...   \n",
       "11553  [Suppose, you, detail, said, I, the, particula...   \n",
       "6666   [Lay, down, your, arms, fellow, men, brethren,...   \n",
       "9120   [An, advanced, guard, gave, information, of, o...   \n",
       "598    [A, paralysis, of, fear, stifled, all, attempt...   \n",
       "\n",
       "                                            text_stemmed  \\\n",
       "10610  [she, howev, shun, societi, and, attach, herse...   \n",
       "11553  [suppos, you, detail, said, i, the, particular...   \n",
       "6666   [lay, down, your, arm, fellow, men, brethren, ...   \n",
       "9120   [an, advanc, guard, gave, inform, of, our, app...   \n",
       "598    [a, paralysi, of, fear, stifl, all, attempt, t...   \n",
       "\n",
       "                                               text_sent  \n",
       "10610  she howev shun societi and attach herself to m...  \n",
       "11553  suppos you detail said i the particular of you...  \n",
       "6666   lay down your arm fellow men brethren pardon s...  \n",
       "9120   an advanc guard gave inform of our approach an...  \n",
       "598      a paralysi of fear stifl all attempt to cri out  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horror_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = cv.fit_transform(horror_data.text_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[:5].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(feature, horror_data.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412665985699693"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EAP'], dtype='<U3')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict(testX[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mnb.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1662,  143,  234],\n",
       "       [ 122, 1125,   92],\n",
       "       [ 117,   69, 1331]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred=y_pred, y_true=testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awantik/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/awantik/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8302349336057201"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1744,  124,  171],\n",
       "       [ 172, 1087,   80],\n",
       "       [ 198,   86, 1233]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_pred=y_pred, y_true=testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=3,\n",
       "                       oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.Series(rf.feature_importances_).sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_fs = feature[:,np.where(res > 0)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(feature_fs, horror_data.author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8171603677221655"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.score(testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli's Naive Bayes\n",
    "* BernoulliNB is designed for binary/boolean features.\n",
    "* If feature data is not binary, BNM will convert into binary\n",
    "* When most of the columns are True/False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Use adult dataset for Bernoulli's NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
